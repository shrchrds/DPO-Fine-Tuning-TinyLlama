# DPO Fine-Tuning TinyLLaMA

This Jupyter Notebook demonstrates how to fine-tune a TinyLLaMA model using Differentiable Prompting Optimization (DPO) on a custom dataset.

## Prerequisites

- Python 3.7 or higher
- PyTorch 1.10.0 or higher
- Transformers 4.25.1 or higher
- PEFT 0.2.0 or higher
- Accelerate 0.15.0 or higher
- Datasets 2.8.0 or higher
- Bitsandbytes 0.35.0 or higher

You can install the required dependencies using the following command:

```!pip install transformers trl peft accelerate datasets bitsandbytes```

## Usage
- Ensure you have the necessary dependencies installed.

- Open the dpo_fine_tuning.ipynb file in your Jupyter Notebook environment.

- Follow the instructions and code examples in the notebook to fine-tune the TinyLLaMA model using DPO on your custom dataset.

- Customize the code as needed to fit your specific requirements.

## Acknowledgements
- This work is based on the research and implementation of Differentiable Prompting Optimization (DPO) by the Anthropic team. The TinyLLaMA model is a lightweight version of the LLaMA model, developed by Meta AI.

This README.md file provides a high-level overview of the `dpo_fine_tuning.ipynb` file, including the prerequisites, usage instructions, and acknowledgements. You can customize this template further to fit your specific project needs.
